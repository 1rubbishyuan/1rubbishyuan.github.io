<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <title>attention到底在做什么 | Home</title>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords" content="">
  <meta name="theme-color" content="#10b981">

  <link rel="canonical" href="http://example.com/2025/07/14/attention%E5%88%B0%E5%BA%95%E5%9C%A8%E5%81%9A%E4%BB%80%E4%B9%88/">

  
    <link rel="shortcut icon" href="/img/page_avatar.png">
  

  <meta name="description" content="前言近期看了anthropic的一些关于attention可解释性的文章，感觉很有意思，试着做了一些总结和思考，也希望给之后继续学习ml system层面对transformer的优化明确一些较为基本的常识，先记录在此。 回顾attentionattention数学形式首先，我们先不要去考虑multi head和mask的事情（后文会慢慢引出），先从最为简单和朴素的attention开始，这样就会">
<meta property="og:type" content="article">
<meta property="og:title" content="attention到底在做什么">
<meta property="og:url" content="http://example.com/2025/07/14/attention%E5%88%B0%E5%BA%95%E5%9C%A8%E5%81%9A%E4%BB%80%E4%B9%88/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="前言近期看了anthropic的一些关于attention可解释性的文章，感觉很有意思，试着做了一些总结和思考，也希望给之后继续学习ml system层面对transformer的优化明确一些较为基本的常识，先记录在此。 回顾attentionattention数学形式首先，我们先不要去考虑multi head和mask的事情（后文会慢慢引出），先从最为简单和朴素的attention开始，这样就会">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-07-14T13:05:00.000Z">
<meta property="article:modified_time" content="2025-07-15T12:03:48.754Z">
<meta property="article:author" content="原嘉锐">
<meta property="article:tag" content="ML基础">
<meta name="twitter:card" content="summary">
  <meta name="generator" content="Hexo 7.3.0">
  <script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2025/07/14/attention%E5%88%B0%E5%BA%95%E5%9C%A8%E5%81%9A%E4%BB%80%E4%B9%88/"},"headline":"attention到底在做什么","description":"前言近期看了anthropic的一些关于attention可解释性的文章，感觉很有意思，试着做了一些总结和思考，也希望给之后继续学习ml system层面对transformer的优化明确一些较为基本的常识，先记录在此。 回顾attentionattention数学形式首先，我们先不要去考虑multi head和mas","datePublished":"2025-07-14T13:05:00.000Z","dateModified":"2025-07-15T12:03:48.754Z","author":{"@type":"Person","name":"原嘉锐"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"http://example.com/img/page_avatar.png"}}}
</script>

  
  
<link rel="stylesheet" href="/css/main.css">

  <style>
    :root {
      --sea-color-primary: #10b981;
    }
  </style>

  
<script src="/js/theme_mode.js"></script>

</head>
<body>
  <header class="sea-header">
    <nav class="sea-nav-wrap">
  <div class="sea-nav-logo" title="">
    <a href="/">Home</a>
  </div>
  <div class="sea-nav-menus">
    <div id="sea-nav-toggle">
      <svg t="1716965724278" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="10878" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M950.857143 768v73.142857c0 20.004571-16.566857 36.571429-36.571429 36.571429H109.714286c-20.004571 0-36.571429-16.566857-36.571429-36.571429v-73.142857c0-20.004571 16.566857-36.571429 36.571429-36.571429h804.571428c20.004571 0 36.571429 16.566857 36.571429 36.571429z m0-292.571429v73.142858c0 20.004571-16.566857 36.571429-36.571429 36.571428H109.714286c-20.004571 0-36.571429-16.566857-36.571429-36.571428v-73.142858c0-20.004571 16.566857-36.571429 36.571429-36.571428h804.571428c20.004571 0 36.571429 16.566857 36.571429 36.571428z m0-292.571428v73.142857c0 20.004571-16.566857 36.571429-36.571429 36.571429H109.714286c-20.004571 0-36.571429-16.566857-36.571429-36.571429V182.857143c0-20.004571 16.566857-36.571429 36.571429-36.571429h804.571428c20.004571 0 36.571429 16.566857 36.571429 36.571429z" p-id="10879"></path></svg>
    </div>

    <div id="sea-nav-dimmer"></div>
<div class="sea-menu-wrap">
  
    <a
      class="sea-menu-link "
      
      href="/archives/"
    >
      归档
    </a>
  
    <a
      class="sea-menu-link "
      
      href="/categories/"
    >
      分类
    </a>
  
    <a
      class="sea-menu-link "
      
      href="/tags/"
    >
      标签
    </a>
  
    <a
      class="sea-menu-link "
      
      href="/messages/"
    >
      留言
    </a>
  
    <a
      class="sea-menu-link "
      
      href="/about/"
    >
      关于
    </a>
  

  <span class="sea-menu-sep">|</span>

  

  <span class="sea-menu-icon" id="sea-theme-dark">
    <svg t="1725413107294" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="10118" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M557.553778 976.355556c-257.265778 0-466.56-207.160889-466.56-464.426667 0-253.923556 206.577778-464.284444 460.501333-464.284445h0.355556c10.766222 0 20.622222 3.953778 25.443555 13.610667 4.878222 9.756444 3.740444 20.394667-2.915555 29.027556-55.722667 72.220444-85.162667 158.108444-85.162667 249.372444 0 225.891556 183.779556 409.386667 409.671111 409.386667l5.248-0.256c10.325333-0.142222 20.977778 5.859556 25.841778 15.644444a28.302222 28.302222 0 0 1-2.915556 30.051556C837.902222 910.08 703.203556 976.355556 557.553778 976.355556zM495.274667 105.016889C299.192889 135.281778 147.882667 306.161778 147.882667 509.809778c0 225.877333 183.779556 409.656889 409.671111 409.656889 108.686222 0 210.403556-42.055111 286.577778-116.977778-231.566222-27.192889-411.804444-224.625778-411.804445-463.36 0-83.427556 21.617778-163.299556 62.947556-234.112z" fill="" p-id="10119"></path><path d="M578.830222 879.132444c-186.865778 0-345.784889-133.418667-377.841778-317.269333a14.222222 14.222222 0 1 1 28.017778-4.878222c29.681778 170.183111 176.810667 293.703111 349.824 293.703111a14.222222 14.222222 0 1 1 0 28.444444zM209.991111 531.2c-7.537778 0-13.838222-6.997333-14.193778-14.606222-0.312889-6.584889-0.483556-13.795556-0.483555-20.465778 0-7.864889 6.357333-14.492444 14.222222-14.492444s14.222222 6.229333 14.222222 14.094222c0 6.229333 0.170667 13.425778 0.455111 19.584 0.369778 7.850667-5.674667 15.886222-13.525333 15.886222h-0.696889z" fill="" p-id="10120"></path><path d="M622.350222 309.930667m-25.344 0a25.344 25.344 0 1 0 50.688 0 25.344 25.344 0 1 0-50.688 0Z" fill="" p-id="10121"></path><path d="M787.072 188.273778m-25.344 0a25.344 25.344 0 1 0 50.688 0 25.344 25.344 0 1 0-50.688 0Z" fill="" p-id="10122"></path><path d="M731.960889 415.303111m-25.344 0a25.344 25.344 0 1 0 50.688 0 25.344 25.344 0 1 0-50.688 0Z" p-id="10123"></path></svg>
  </span>
  <span class="sea-menu-icon" id="sea-theme-light">
    <svg t="1725410359322" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4274" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M512 768c-141.376 0-256-114.624-256-256s114.624-256 256-256 256 114.624 256 256-114.624 256-256 256z m0-85.333333a170.666667 170.666667 0 1 0 0-341.333334 170.666667 170.666667 0 0 0 0 341.333334zM469.333333 85.333333a42.666667 42.666667 0 1 1 85.333334 0v85.333334a42.666667 42.666667 0 1 1-85.333334 0V85.333333z m0 768a42.666667 42.666667 0 1 1 85.333334 0v85.333334a42.666667 42.666667 0 1 1-85.333334 0v-85.333334zM85.333333 554.666667a42.666667 42.666667 0 1 1 0-85.333334h85.333334a42.666667 42.666667 0 1 1 0 85.333334H85.333333z m768 0a42.666667 42.666667 0 1 1 0-85.333334h85.333334a42.666667 42.666667 0 1 1 0 85.333334h-85.333334zM161.834667 222.165333a42.666667 42.666667 0 0 1 60.330666-60.330666l64 64a42.666667 42.666667 0 0 1-60.330666 60.330666l-64-64z m576 576a42.666667 42.666667 0 0 1 60.330666-60.330666l64 64a42.666667 42.666667 0 0 1-60.330666 60.330666l-64-64z m-515.669334 64a42.666667 42.666667 0 0 1-60.330666-60.330666l64-64a42.666667 42.666667 0 0 1 60.330666 60.330666l-64 64z m576-576a42.666667 42.666667 0 0 1-60.330666-60.330666l64-64a42.666667 42.666667 0 0 1 60.330666 60.330666l-64 64z" p-id="4275"></path></svg>
  </span>

  <span id="sea-menu-close-icon">
    <svg t="1725435896874" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4408" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M556.8 512l265.6-265.6c12.8-12.8 12.8-32 0-44.8s-32-12.8-44.8 0L512 467.2 246.4 201.6c-12.8-12.8-32-12.8-44.8 0s-12.8 32 0 44.8l265.6 265.6-265.6 265.6c-12.8 12.8-12.8 32 0 44.8 6.4 6.4 12.8 9.6 22.4 9.6s16-3.2 22.4-9.6l265.6-265.6 265.6 265.6c6.4 6.4 16 9.6 22.4 9.6s16-3.2 22.4-9.6c12.8-12.8 12.8-32 0-44.8L556.8 512z" p-id="4409"></path></svg>
  </span>
</div>
  </div>
</nav>
  </header>
  <main id="sea-main-wrapper" data-pagefind-body>
    <article class="sea-page-card-wrapper">
  <header class="sea-article-header">
    <h1 class="sea-article-title">attention到底在做什么</h1>
    
      <div class="sea-post-meta sea-post-meta__center">
        <div class="sea-post-time">
  <svg t="1716964550804" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2621" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M805.49888 981.49888l-602.3168-0.76288c-86.59456-8.192-154.56768-81.3056-154.56768-170.01472L48.6144 291.73248c0-94.1568 76.60032-170.75712 170.7776-170.75712l586.10176 0c94.1568 0 170.73152 76.60032 170.73152 170.75712L976.22528 810.7008C976.2304 904.87296 899.63008 981.49888 805.49888 981.49888L805.49888 981.49888zM219.3664 190.57152c-55.79776 0-101.20192 45.38368-101.20192 101.18144l0 518.96832c0 55.79776 45.40416 101.20704 101.20192 101.20704l586.13248 0c55.77728 0 101.16096-45.40928 101.16096-101.20704L906.65984 291.73248c0-55.79776-45.38368-101.18656-101.16096-101.18656L219.3664 190.54592 219.3664 190.57152zM698.84416 290.51904c-25.60512 0-46.38208-20.77696-46.38208-46.38208l0-158.6688c0-25.6 20.77696-46.38208 46.38208-46.38208 25.6 0 46.38208 20.78208 46.38208 46.38208L745.22624 244.1216C745.22624 269.7472 724.46976 290.51904 698.84416 290.51904L698.84416 290.51904zM315.65824 290.51904c-25.60512 0-46.38208-20.77696-46.38208-46.38208l0-158.6688c0-25.6 20.77696-46.38208 46.38208-46.38208 25.6 0 46.38208 20.78208 46.38208 46.38208L362.04032 244.1216C362.04032 269.7472 341.28896 290.51904 315.65824 290.51904L315.65824 290.51904zM534.8864 794.78784l-44.27264 0c-25.6 0-46.38208-20.77696-46.38208-46.38208 0-25.6 20.78208-46.38208 46.38208-46.38208l44.27264 0c25.6 0 46.38208 20.78208 46.38208 46.38208C581.26848 774.01088 560.4864 794.78784 534.8864 794.78784L534.8864 794.78784zM930.79552 452.608 121.24672 452.608c-25.60512 0-46.38208-20.78208-46.38208-46.38208 0-25.60512 20.77696-46.38208 46.38208-46.38208l809.5744 0c25.6 0 46.38208 20.77696 46.38208 46.38208C977.2032 431.82592 956.42624 452.608 930.79552 452.608L930.79552 452.608zM327.92576 649.03168l-44.27264 0c-25.6 0-46.38208-20.78208-46.38208-46.38208 0-25.60512 20.78208-46.38208 46.38208-46.38208l44.27264 0c25.6 0 46.38208 20.77696 46.38208 46.38208C374.30784 628.25472 353.52576 649.03168 327.92576 649.03168L327.92576 649.03168zM534.8864 649.03168l-44.27264 0c-25.6 0-46.38208-20.78208-46.38208-46.38208 0-25.60512 20.78208-46.38208 46.38208-46.38208l44.27264 0c25.6 0 46.38208 20.77696 46.38208 46.38208S560.4864 649.03168 534.8864 649.03168L534.8864 649.03168zM741.27872 649.03168l-44.26752 0c-25.60512 0-46.38208-20.78208-46.38208-46.38208 0-25.60512 20.77696-46.38208 46.38208-46.38208l44.26752 0c25.60512 0 46.38208 20.77696 46.38208 46.38208C787.6608 628.25472 766.90944 649.03168 741.27872 649.03168L741.27872 649.03168zM327.92576 794.78784l-44.27264 0c-25.6 0-46.38208-20.77696-46.38208-46.38208 0-25.6 20.78208-46.38208 46.38208-46.38208l44.27264 0c25.6 0 46.38208 20.78208 46.38208 46.38208C374.30784 774.01088 353.52576 794.78784 327.92576 794.78784L327.92576 794.78784zM741.27872 794.78784l-44.26752 0c-25.60512 0-46.38208-20.77696-46.38208-46.38208 0-25.6 20.77696-46.38208 46.38208-46.38208l44.26752 0c25.60512 0 46.38208 20.78208 46.38208 46.38208C787.6608 774.01088 766.90944 794.78784 741.27872 794.78784L741.27872 794.78784z" p-id="2622"></path></svg>
  <time datetime="2025-07-14T13:05:00.000Z">2025-07-14</time>
</div>
        
  <div class="sea-post-categories">
    <svg t="1716964680422" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4550" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M810.666667 85.333333a85.333333 85.333333 0 0 1 85.333333 85.333334v152.021333c36.821333 9.493333 64 42.88 64 82.645333v405.333334a128 128 0 0 1-128 128H192a128 128 0 0 1-128-128V298.666667a85.376 85.376 0 0 1 64-82.645334V170.666667a85.333333 85.333333 0 0 1 85.333333-85.333334h597.333334zM128.149333 296.170667L128 298.666667v512a64 64 0 0 0 60.245333 63.893333L192 874.666667h640a64 64 0 0 0 63.893333-60.245334L896 810.666667V405.333333a21.333333 21.333333 0 0 0-18.837333-21.184L874.666667 384H638.165333l-122.069333-101.717333a21.333333 21.333333 0 0 0-10.688-4.736l-2.986667-0.213334H149.333333a21.333333 21.333333 0 0 0-21.184 18.837334zM535.189333 213.333333l127.978667 106.666667H832V170.666667a21.333333 21.333333 0 0 0-18.837333-21.184L810.666667 149.333333H213.333333a21.333333 21.333333 0 0 0-21.184 18.837334L192 170.666667v42.666666h343.168z" p-id="4551"></path></svg>
    <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

        
  <div class="sea-post-tags">
    <svg t="1716964811431" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6117" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M384 977.152c-20.5312 0-39.8336-7.9872-54.3232-22.4256l-260.4032-260.4032c-14.4896-14.4896-22.4256-33.7408-22.4256-54.3232s7.9872-39.8336 22.4256-54.3232l439.6032-439.6032c24.9344-24.9344 70.2464-43.7248 105.5232-43.7248h230.4c42.3424 0 76.8 34.4576 76.8 76.8v230.4c0 35.2256-18.7904 80.5888-43.6736 105.5232l-439.6032 439.6032a76.1856 76.1856 0 0 1-54.3232 22.4256zM614.4 153.6c-21.248 0-54.272 13.6704-69.2736 28.7232l-439.6032 439.6032c-4.8128 4.8128-7.424 11.2128-7.424 18.1248s2.6624 13.312 7.424 18.0736l260.4032 260.4032c4.8128 4.8128 11.2128 7.424 18.1248 7.424s13.312-2.6624 18.1248-7.424l439.6032-439.6032c15.0016-15.0016 28.7232-48.0768 28.7232-69.3248V179.2a25.6 25.6 0 0 0-25.6-25.6h-230.4z" p-id="6118"></path><path d="M742.4 358.4c-42.3424 0-76.8-34.4576-76.8-76.8S700.0576 204.8 742.4 204.8s76.8 34.4576 76.8 76.8S784.7424 358.4 742.4 358.4z m0-102.4a25.6 25.6 0 1 0 0 51.2 25.6 25.6 0 0 0 0-51.2z" p-id="6119"></path></svg>
    <a class="tag-link" href="/tags/ML%E5%9F%BA%E7%A1%80/" rel="tag">ML基础</a>
  </div>

      </div>
    
  </header>
  <div class="sea-doc">
    
    <div class="sea-article-content">
      <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>近期看了anthropic的一些关于attention可解释性的文章，感觉很有意思，试着做了一些总结和思考，也希望给之后继续学习ml system层面对transformer的优化明确一些较为基本的常识，先记录在此。</p>
<h2 id="回顾attention"><a href="#回顾attention" class="headerlink" title="回顾attention"></a>回顾attention</h2><h4 id="attention数学形式"><a href="#attention数学形式" class="headerlink" title="attention数学形式"></a>attention数学形式</h4><p>首先，我们先不要去考虑multi head和mask的事情（后文会慢慢引出），先从最为<strong>简单和朴素的attention开始</strong>，这样就会发现attention在数学上的表达是简单漂亮且有具体语义的，主要由四个部分组成，一个输入x(在不考虑batch_size的情况下，其维度为(sequence_length,d_model))，分别用于生成Q(query)、K(key)、V(value)的矩阵$W_q$、$W_k$、$W_v$，这些就是attention最为主要的几个变量了，Q与K负责attention的计算，其值代表着q位置对k位置信息的关注程度，V则代表着该位置所聚合的信息，基于这些变量，我们就得到了attention的计算公式：<br>$$Attenion(Q,K,V)&#x3D;\mathcal{softmax}(\frac{QK^T}{\sqrt{d_k}})V$$<br>其中$d_k$就是query和key向量的维度，被广为认可的解释是为了防止点积过大带来的梯度计算问题，$d_k$只是一个常数，为了方便和美观，在后面的数学表达中忽略掉$d_k$。</p>
<p>我们将Q,K,V表达为分块向量的形式，也即<br>$$K &#x3D; \begin{bmatrix}<br>k_1 \<br>k_2 \<br>\vdots \<br>k_T<br>\end{bmatrix},<br>\quad<br>Q &#x3D; \begin{bmatrix}<br>q_1 \<br>q_2 \<br>\vdots \<br>q_T<br>\end{bmatrix},<br>\quad<br>V &#x3D; \begin{bmatrix}<br>v_1 \<br>v_2 \<br>\vdots \<br>v_T<br>\end{bmatrix}$$<br>其中$k_i,q_i$的维度是$d_k$,$v_i$的维度是$d_v$,$T$就是sequence的长度。</p>
<p>随后我们计算attention，得到<br>$$<br>softmax(QK^T)V &#x3D; \left[<br>\begin{array}{cccc}<br>q_1 \cdot k_1 &amp; q_1 \cdot k_2 &amp; \cdots &amp; q_1 \cdot k_T \<br>q_2 \cdot k_1 &amp; q_2 \cdot k_2 &amp; \cdots &amp; q_2 \cdot k_T \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>q_T \cdot k_1 &amp; q_T \cdot k_2 &amp; \cdots &amp; q_T \cdot k_T<br>\end{array}<br>\right]  \begin{bmatrix}<br>v_1 \<br>v_2 \<br>\vdots \<br>v_T<br>\end{bmatrix}<br>$$<br>这就是最朴素的attention结果，在实际使用中，尤其现在主流的大模型结构都已经是decoder-only的结构，所以多为带<strong>mask的版本</strong>，而所谓带mask的版本就是不允许前文中的token关注到后文中的token，其数学形式就是<br>$$softmax(QK^T)V &#x3D; \left[<br>\begin{array}{cccc}<br>q_1 \cdot k_1 &amp; 0 &amp; \cdots &amp; 0 \<br>q_2 \cdot k_1 &amp; q_2 \cdot k_2 &amp; \cdots &amp; 0 \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>q_T \cdot k_1 &amp; q_T \cdot k_2 &amp; \cdots &amp; q_T \cdot k_T<br>\end{array}<br>\right]  \begin{bmatrix}<br>v_1 \<br>v_2 \<br>\vdots \<br>v_T<br>\end{bmatrix} &#x3D; \begin{bmatrix}<br>(q_1 \cdot k_1)v_1 \<br>(q_2 \cdot k_1)v_1 + (q_2 \cdot k_2)v_2 \<br>\vdots \<br>(q_T \cdot k_1)v_1 + \cdots + (q_T \cdot k_T)v_T<br>\end{bmatrix}<br>$$</p>
<div style="border:1px solid #ccc; padding:10px; border-radius:5px;">
 <strong>KV cache</strong> </br>
 从带mask的attention得到的结果，我们可以解释为什么推理的时候需要kv cache，而q cache却并不被需要。</br>
 首先我们需要有一个概念，attention计算的结果是一个维度为(sequence_length,d_model)的向量，在做推理的时候，只会把最后一个维度的d_model向量用于计算最终的logits(用于计算词表中各个词的概率)，又结合推理过程是Next Token Prediction的形式，token是一个接着一个decode出来的，所以实际上只需要关注上文中带mask的attention计算结果最后一行的向量，可以看到这里会用到1到T所有的k和v，但是对于q则只会使用T位置的，故不存在q cache。</br>
 而对于训练，则会是每一个位置都会用于计算logits然后计算loss，且训练时全文可见，可以并行计算，不存在kv cache的问题。
</div>

<h4 id="multi-heads"><a href="#multi-heads" class="headerlink" title="multi heads"></a>multi heads</h4><p>那么以上就是最为朴素的attention结构，也可以被称为一个head，那自然就会有<strong>multi heads</strong>的attention结构，它可以使得模型可以关注到更多层面的信息。首先输出x(维度为d_model)会被切割为若干个等长的vector，常见的有32、64等，然后每一个切割后的vector会由一个head计算，最终得到的所有vector会被直接concate在一起，并由一个$W_O$做一次线性变化作为输出，这就是multi head attention的结构。</p>
<h2 id="从incontext-learning能力出发分析attention到底做了什么"><a href="#从incontext-learning能力出发分析attention到底做了什么" class="headerlink" title="从incontext learning能力出发分析attention到底做了什么"></a>从incontext learning能力出发分析attention到底做了什么</h2><h4 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h4><p>attention结构被广泛应用在transformer架构中，我们的分析聚焦于现在主流的decoder-only的transformer结构，同时为了将关注点放在attention上并简化模型，我们忽略掉MLP，bias等。</p>
<p>从最简单的bigram词统计开始到其他更复杂的next token prediction的方法，对于下一个词的预测其实都可以理解为是对前文信息的汇总，bigram是容易理解的，它通过词统计来完成“训练”，然后根据距离待预测位置最近的一个token的信息来得到输出的token。相对复杂的attention通过Q,K点乘得到的Attention矩阵来代表关注程度，V是sequence每个token位置所包含的信息,Attention矩阵与V相乘则是信息聚合的过程，信息聚合的结果就是attention的输出。完全基于统计的bigram对于统计中从未出现过的新词往往无能为力，但基于attention的transformer却涌现出来一个比较有趣的能力——incontext learning能力，也即从未在训练数据中出现过的词，transformer仍然能够理解其含义并用于预测中，这体现出了基于attention的transformer信息聚合能力的强大，接下来我们将从零层attention结构的transformer开始分析incontext learning能力的涌现，感受attention聚集信息的过程。</p>
<h4 id="几个数学建模"><a href="#几个数学建模" class="headerlink" title="几个数学建模"></a>几个数学建模</h4><h4 id="Zero-Layer-Transformer"><a href="#Zero-Layer-Transformer" class="headerlink" title="Zero Layer Transformer"></a>Zero Layer Transformer</h4><h4 id="One-Layer-Transformer"><a href="#One-Layer-Transformer" class="headerlink" title="One Layer Transformer"></a>One Layer Transformer</h4><h4 id="Two-Layer-Transformer"><a href="#Two-Layer-Transformer" class="headerlink" title="Two Layer Transformer"></a>Two Layer Transformer</h4><h4 id="incontext-learning能力评测"><a href="#incontext-learning能力评测" class="headerlink" title="incontext learning能力评测"></a>incontext learning能力评测</h4><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>A Mathematical Framework for Transformer Circuits   <a target="_blank" rel="noopener" href="https://transformer-circuits.pub/2021/framework/index.html">https://transformer-circuits.pub/2021/framework/index.html</a></p>
<p>Incontext learning and induction heads   <a target="_blank" rel="noopener" href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html</a></p>

    </div>
  </div>

  
    
  <div class="sea-prev-next-wrapper">
    
    
      <div class="next">
        <a class="link" href="/2025/07/08/%E8%B0%83%E8%89%B2%E6%97%A5%E5%BF%97_%E7%BB%8F%E5%85%B8%E9%A3%8E%E6%A0%BC/">
          经典调色风格
        </a>
        <svg t="1725418993065" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6832" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M731.733333 480l-384-341.333333c-17.066667-14.933333-44.8-14.933333-59.733333 4.266666-14.933333 17.066667-14.933333 44.8 4.266667 59.733334L640 512 292.266667 821.333333c-17.066667 14.933333-19.2 42.666667-4.266667 59.733334 8.533333 8.533333 19.2 14.933333 32 14.933333 10.666667 0 19.2-4.266667 27.733333-10.666667l384-341.333333c8.533333-8.533333 14.933333-19.2 14.933334-32s-4.266667-23.466667-14.933334-32z" p-id="6833"></path></svg>
      </div>
    
  </div>

  
</article>



<script defer>
  document.addEventListener('DOMContentLoaded', function () {
    const toggleIcon = document.querySelector('.sea-article-catalog-title .sea-svg-icon');
    const tocContent = document.querySelector('.sea-article-catalog > .toc');
    if (toggleIcon && tocContent) {
      toggleIcon.addEventListener('click', function () {
        tocContent.classList.toggle('sea-article-catalog-show');
        toggleIcon.classList.toggle('sea-svg-icon-rotate');
      });
    }
  });
</script>
  </main>
  <footer id="sea-footer-container">
  <div class="sea-footer-row">
    <div class="sea-footer-menu-link">
      
    </div>
  </div>
  
  
  <div class="sea-footer-row">
    <div class="sea-footer-copyright">
      <span>©</span>
      
        2025
      
      <span>·</span>
      原嘉锐
    </div>
    <span class="split-line">|</span>
    <div class="sea-footer-theme-by">
      Theme by <a class="theme" href="https://github.com/hai-zou/hexo-theme-sea" target="_blank">Sea</a>
    </div>
  </div>
</footer>

  
<script src="/js/main.js" defer></script>


<script src="/js/theme.js" defer></script>

</body>
</html>